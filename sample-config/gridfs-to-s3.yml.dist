#
# MongoDB GridFS -> Amazon S3 copy script configuration
#

---

# Lock file
#
# The script will create the lock file when started; if the lock file is
# present, another instance of the script will refuse to start. This is to
# prevent multiple instances of the script from doing the same thing.
lock_file       : "gridfs-to-s3.lock"

# How many worker threads to spawn
worker_threads  : 32

# How many jobs to add to the job pool at once
job_chunk_size  : 512

# Various means to copy objects to / from
connectors:

    # MongoDB GridFS connection settings
    "mongodb_gridfs":
        type        : "GridFS"
        host        : "localhost" 
        port        : "27017" 
        database    : "my_files"
        # no timeout by default (wait forever because of the "recv timed out" errors)
        timeout     : -1
        # File in which the last copied filename from GridFS is stored
        last_copied_file    : "copy-kvs-gridfs.last"

    # Amazon S3 connection settings
    "amazon_s3":
        type                    : "AmazonS3"
        access_key_id           : "AKIAIOSFODNN7EXAMPLE"
        secret_access_key       : "wJalrXUtnFEMI/K7MDENG/bPxRfiCYzEXAMPLEKEY"
        bucket_name             : "backup-gridfs"
        # If set, files in S3 will be saved as "<downloads_directory_name>/<filename>"
        # instead of just "<filename>"; may be empty
        directory_name          : "backup"
        # will try 3 times within 19 seconds:
        timeout                 : 60
        # Use SSL (1/0)?
        use_ssl                 : 0
        # Whether to check if the file exists (by sending a HEAD request) before doing
        # anything else.
        # Setting to "yes" will increase both the script verbosity (useful for
        # debugging and paranoid types) and Amazon S3 costs (because HEAD requests
        # cost too!)
        head_before:
            # HEAD before PUTting a file (1/0)
            put                 : 0
            # HEAD before GETting a file (1/0)
            get                 : 0
            # HEAD before DELETing a file (1/0)
            delete              : 0
        # Overwrite the file if it already exists (1/0)?
        # (if you set it to 0, "head_before->put" will be enabled because in order
        # to ensure that the file doesn't get overwritten, the script has to check
        # whether or not the file exists first)
        overwrite               : 1
        # File in which the last copied filename from S3 is stored
        last_copied_file    : "copy-kvs-s3.last"

    # PostgreSQL BLOB connection settings
    "postgres_blob":
        type            : "PostgresBLOB"
        host            : "localhost"
        port            : 5432
        username        : "backup"
        password        : "backup1"
        database        : "backup"
        schema          : "public"
        # Table in which BLOBs are being stored
        table           : "binary_blobs"
        # Column with object IDs (filenames)
        id_column       : "object_id"
        # Column of type "bytea" in said table with BLOBs
        data_column     : "data"
        # File in which the last copied filename from PostgreSQL BLOBs is stored
        last_copied_file    : "copy-kvs-postgres.last"
